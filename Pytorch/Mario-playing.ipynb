{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random,datetime,os\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import TensorDictReplayBuffer,LazyMemmapStorage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Environment\n",
    "### Initialize Environment\n",
    "In Mario, the environment consists of tubes, mushrooms and other components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Super Mario environment\n",
    "if gym.__version__<'0.26':\n",
    "    env=gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\",new_step_api=True)\n",
    "else:\n",
    "    env=gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\",render_mode='rgb',apply_api_compatibility=True)\n",
    "\n",
    "# Limit the action_space to\n",
    "# 0. walk right\n",
    "# 1.jump right\n",
    "env = JoypadSpace(env,[[\"right\"],[\"right\",\"A\"]])\n",
    "\n",
    "env.reset()\n",
    "next_state,reward,done,trunc,info=env.step(action=0)\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess Environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SkipFrame(gym.wrappers):\n",
    "    def __init__(self,env,skip):\n",
    "        \"\"\"Return only every 'skip'-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self.env=env\n",
    "        self._skip=skip\n",
    "\n",
    "    def step(self,action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward=0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs,r,done,trunc,info=self.env.step(action)\n",
    "            total_reward+=r\n",
    "            if done:\n",
    "                break\n",
    "        return obs,total_reward,done,trunc,info"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self,env):\n",
    "        super().__init__(env)\n",
    "        obs_shape=self.observation_space.shape[:2]\n",
    "        self.observation_space=Box(low=0,high=255,shape=obs_shape,dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self,observation):\n",
    "        # permute [H,W,C] array to [C,H,W] tensor\n",
    "        observation=np.transpose(observation,(2,0,1))\n",
    "        observation=torch.tensor(observation.copy(),dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation=self.permute_orientation(observation)\n",
    "        transform=T.Grayscale()\n",
    "        observation=transform(observation)\n",
    "        return observation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self,env,shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape,int):\n",
    "            self.shape=(shape,shape)\n",
    "        else:\n",
    "            self.shape=tuple(shape)\n",
    "\n",
    "        obs_shape=self.shape+self.observation_space.shape[:2]\n",
    "        self.observation_space=Box(low=0,high=255,shape=obs_shape,dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms=T.Compose(\n",
    "            [T.Resize(self.shape,antialias=True),T.Normalize(0,255)]\n",
    "        )\n",
    "        observation=transforms(observation).squeeze(0)\n",
    "        return observation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Apply Wrappers to environment\n",
    "env=SkipFrame(env,skip=4)  # 帧跳过\n",
    "env=GrayScaleObservation(env)  # 灰度化\n",
    "env=ResizeObservation(env,shape=84)  # 调整大小\n",
    "if gym.__version__<'0.26':\n",
    "    env=FrameStack(env,num_stack=4,new_step_api=True)\n",
    "else:\n",
    "    env=FrameStack(env,num_stack=4)  # 帧堆栈\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agent\n",
    "We create a class `Mario` to represent our agent in the game. Mario should be able to:\n",
    "* **Act** according to the optimal action policy based on the current state(of the environment).\n",
    "* **Remember** experiences. Experience=(current state,current action,reward,next state).\n",
    "* **Learn** a better action policy over time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def act(self,state):\n",
    "        \"\"\"Given a state, choose an epsilon-greedy action\"\"\"\n",
    "        pass\n",
    "\n",
    "    def cache(self,experience):\n",
    "        \"\"\"Add the experience to memory\"\"\"\n",
    "        pass\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"Sample experiences from memory\"\"\"\n",
    "        pass\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Update online action value (Q) function with a batch of experiences\"\"\"\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Act\n",
    "For any given state, an agent can choose to do the most optimal action(**exploit**) or a random action(**explore**)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO\n",
    "class Mario:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}